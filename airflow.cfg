[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
dags_folder = /usr/local/airflow/dags

# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = /usr/local/airflow/logs

[logging]
# Log format
log_format = %(levelname)s - %(message)s

# Log levels
logging_level = INFO

# Airflow can store logs remotely in AWS S3 or Google GCS. Users
# must supply a remote location URL (starting with either 's3://...' or
# 'gs://...') and an Airflow connection id that provides access to the storage
# Airflow will automatically use boto3 or google client libraries to interact with
# the storage backend.
remote_logging = False

# Airflow can store logs remotely in S3
remote_base_log_folder =

# The default task logs are stored in the folder
task_log_prefix_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}
